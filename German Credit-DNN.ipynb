{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German Credit Analysis, Using DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the project\n",
    "\n",
    "This purpose of this project is to replicate the findings in \"Combining Feature Selection and Neural Networks for Solving Classification Problems,\" O'Dea, P., Griffith, J., O'Riordan, C. This paper used the German Credit data set and combined feature selection and Neural Networks to solve classification problems.\n",
    "\n",
    "This approach took the form of two phases. The first phase was to use Information Theory to determine the attributes of the German credit data set that are the most important in classifiying the data record as good credit or bad credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Mining Methods and their Applications\n",
    "\n",
    "Data mining is deployed in many different \"classification and association rules, and to item-set recognition and sequential pattern recoginition problems\" (O'Dea, p. 1). Many of these methods are outlined in Provost & Fawcett pages 20-23.\n",
    "\n",
    "When deploying classification methods, the key objective is to predict which class an item belong to based on a set of attributes (Provost & Fawcett, p. 20). Applications of classification involve prediciting customer churn, or in classifying what class an item belongs to. Examples of methods involved in classifications are Neural Networks, score cards, and decision trees.\n",
    "\n",
    "Regression, or \"value estimation\" (Provost & Fawcett, p. 21) is a technique that attempts to predict the numerical value based on past data. The technique uses a linear mathematical model (often using the least squares approach), that can predict the dependent variable from an independent variable. Applications of regression models include predicting the temperature on a given day based on previous days (or seasonal data).\n",
    "\n",
    "Similarity matching used known data about items and attempts to identify similar items. Applications of similarity matching include finding customers that are similar to your best customer.\n",
    "\n",
    "Clustering methods is an unsupervisied (no target attribute), that attempts to cluster or group like items together. Applications of clustering include finding segments in groups of customers.\n",
    "\n",
    "Co-occurrence grouping is a data mining method that attempts to associate items based on their transactions. It is also called assiaction rule discovery or market-basket analysis. An application of co-occurrence grouping is to suggest products to customers based on the items they have in their shopping cart. A famous example of co-occurrence grouping is the diapers and beer association (Whitehorn, 2006).\n",
    "\n",
    "Profiling is a data mining method that tries to describe the behavior of individuals. It is most useful in \"fraud detection and monitoring intrusion detection\" (Provost & Fawcett, p. 22).\n",
    "\n",
    "Link prediction attempt to determine whether or not a link between items should exist based on associated other links. A common applicaiton of link prediction is friend sugestions in Facebook.\n",
    "\n",
    "Data reduction is a data mining method that attempts to reduce the complexity and volume of a large data set to a more manageable size by pruning the less important information. Data reduction often results in data loss, but at the gain of performance and easier understanding of the data set involved.\n",
    "\n",
    "Causal modeling tries to find links between events and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques and Approaches\n",
    "\n",
    "The techniques used in this model include data segmentation, data reduction, and classification. The first phase of the project is to determine the most important attributes to use when training and testing the model, and reduce the data set to those attributes. This is done using Information Theory.\n",
    "\n",
    "The second phase is to process the reduced data set into a form that can be easily used by the model. This is done by binning the data sets for numerical data into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Industry Standard Process for Data Mining (CRISP-DM)\n",
    "\n",
    "We need a process for data mining in order to have a reasonable amount of “consistency, repeatability, and objectiveness” (Provost, 2013, p. 27) of the outcomes. The CRISP-DM method is one of the most popular data mining processes used in the industry. The CRISP-DM process is an iterative process. Each iteration helps to inform about the data.\n",
    "\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "The business understanding defines the “problem to be solved” (p. 28). The problem here is a feature selection and classification problem: Using the German dataset we need to predict if an applicant is a “good or bad credit risk” (O’Dae, Griffith, O’Riordan; p. 6).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from math import log\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "The data is composed of twenty attributes. Using entropy and information gain, seven attributes were selected as the most useful in determining the credit worthiness of a candidate based on information gain. The attributes selected were: the status of existing checking account, the credit duration in months, credit history, credit amount, savings accounts/bonds, housing (rent, own, for free), and whether or not the person is a foreign worker.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "german = pd.read_csv('MGMT635_GermanCreditData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings</th>\n",
       "      <th>employment_duration</th>\n",
       "      <th>installment_rate</th>\n",
       "      <th>personal_status</th>\n",
       "      <th>debtors</th>\n",
       "      <th>...</th>\n",
       "      <th>property</th>\n",
       "      <th>age</th>\n",
       "      <th>installment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>liable_people</th>\n",
       "      <th>telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>43</td>\n",
       "      <td>1169</td>\n",
       "      <td>65</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>93</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>67</td>\n",
       "      <td>143</td>\n",
       "      <td>152</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>32</td>\n",
       "      <td>43</td>\n",
       "      <td>5951</td>\n",
       "      <td>61</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>22</td>\n",
       "      <td>143</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>191</td>\n",
       "      <td>201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>46</td>\n",
       "      <td>2096</td>\n",
       "      <td>61</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>49</td>\n",
       "      <td>143</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>7882</td>\n",
       "      <td>61</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>103</td>\n",
       "      <td>...</td>\n",
       "      <td>122</td>\n",
       "      <td>45</td>\n",
       "      <td>143</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>4870</td>\n",
       "      <td>61</td>\n",
       "      <td>73</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>124</td>\n",
       "      <td>53</td>\n",
       "      <td>143</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "      <td>201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   status  duration  credit_history  purpose  credit_amount  savings  \\\n",
       "0      11         6              34       43           1169       65   \n",
       "1      12        48              32       43           5951       61   \n",
       "2      14        12              34       46           2096       61   \n",
       "3      11        42              32       42           7882       61   \n",
       "4      11        24              33       40           4870       61   \n",
       "\n",
       "   employment_duration  installment_rate  personal_status  debtors   ...    \\\n",
       "0                   75                 4               93      101   ...     \n",
       "1                   73                 2               92      101   ...     \n",
       "2                   74                 2               93      101   ...     \n",
       "3                   74                 2               93      103   ...     \n",
       "4                   73                 3               93      101   ...     \n",
       "\n",
       "   property  age  installment_plans  housing  existing_credits  job  \\\n",
       "0       121   67                143      152                 2  173   \n",
       "1       121   22                143      152                 1  173   \n",
       "2       121   49                143      152                 1  172   \n",
       "3       122   45                143      153                 1  173   \n",
       "4       124   53                143      153                 2  173   \n",
       "\n",
       "   liable_people  telephone  foreign_worker  target  \n",
       "0              1        192             201       1  \n",
       "1              1        191             201       2  \n",
       "2              2        191             201       1  \n",
       "3              2        191             201       1  \n",
       "4              2        191             201       2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of attributes: Categorical which are in dictionary 1, and numerical which is in dictionary 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_dict = {1:['status', 'credit_history', 'purpose', 'savings',\n",
    "                    'employment_duration', 'personal_status', 'debtors','property', 'installment_plans', 'housing', \n",
    "                    'job', 'telephone', 'foreign_worker'], \n",
    "                  2:['duration', 'credit_amount', 'installment_rate', 'residence', 'age', 'existing_credits',\n",
    "                     'liable_people']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to get insights into the data set is to get some basic statistics. Below is a display of some basic statisitcal information which will tell us things such as mean, standard deviation, minimum and maximum values. For categorical data, the mode would be a useful statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings</th>\n",
       "      <th>employment_duration</th>\n",
       "      <th>installment_rate</th>\n",
       "      <th>personal_status</th>\n",
       "      <th>debtors</th>\n",
       "      <th>...</th>\n",
       "      <th>property</th>\n",
       "      <th>age</th>\n",
       "      <th>installment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>liable_people</th>\n",
       "      <th>telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.577000</td>\n",
       "      <td>20.903000</td>\n",
       "      <td>32.54500</td>\n",
       "      <td>47.148000</td>\n",
       "      <td>3271.258000</td>\n",
       "      <td>62.105000</td>\n",
       "      <td>73.384000</td>\n",
       "      <td>2.973000</td>\n",
       "      <td>92.68200</td>\n",
       "      <td>101.145000</td>\n",
       "      <td>...</td>\n",
       "      <td>122.358000</td>\n",
       "      <td>35.546000</td>\n",
       "      <td>142.675000</td>\n",
       "      <td>151.929000</td>\n",
       "      <td>1.407000</td>\n",
       "      <td>172.904000</td>\n",
       "      <td>1.155000</td>\n",
       "      <td>191.404000</td>\n",
       "      <td>201.037000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.257638</td>\n",
       "      <td>12.058814</td>\n",
       "      <td>1.08312</td>\n",
       "      <td>40.095333</td>\n",
       "      <td>2822.736876</td>\n",
       "      <td>1.580023</td>\n",
       "      <td>1.208306</td>\n",
       "      <td>1.118715</td>\n",
       "      <td>0.70808</td>\n",
       "      <td>0.477706</td>\n",
       "      <td>...</td>\n",
       "      <td>1.050209</td>\n",
       "      <td>11.375469</td>\n",
       "      <td>0.705601</td>\n",
       "      <td>0.531264</td>\n",
       "      <td>0.577654</td>\n",
       "      <td>0.653614</td>\n",
       "      <td>0.362086</td>\n",
       "      <td>0.490943</td>\n",
       "      <td>0.188856</td>\n",
       "      <td>0.458487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91.00000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1365.500000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>92.00000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2319.500000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>93.00000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>3972.250000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>93.00000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>18424.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>94.00000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            status     duration  credit_history      purpose  credit_amount  \\\n",
       "count  1000.000000  1000.000000      1000.00000  1000.000000    1000.000000   \n",
       "mean     12.577000    20.903000        32.54500    47.148000    3271.258000   \n",
       "std       1.257638    12.058814         1.08312    40.095333    2822.736876   \n",
       "min      11.000000     4.000000        30.00000    40.000000     250.000000   \n",
       "25%      11.000000    12.000000        32.00000    41.000000    1365.500000   \n",
       "50%      12.000000    18.000000        32.00000    42.000000    2319.500000   \n",
       "75%      14.000000    24.000000        34.00000    43.000000    3972.250000   \n",
       "max      14.000000    72.000000        34.00000   410.000000   18424.000000   \n",
       "\n",
       "           savings  employment_duration  installment_rate  personal_status  \\\n",
       "count  1000.000000          1000.000000       1000.000000       1000.00000   \n",
       "mean     62.105000            73.384000          2.973000         92.68200   \n",
       "std       1.580023             1.208306          1.118715          0.70808   \n",
       "min      61.000000            71.000000          1.000000         91.00000   \n",
       "25%      61.000000            73.000000          2.000000         92.00000   \n",
       "50%      61.000000            73.000000          3.000000         93.00000   \n",
       "75%      63.000000            75.000000          4.000000         93.00000   \n",
       "max      65.000000            75.000000          4.000000         94.00000   \n",
       "\n",
       "           debtors     ...          property          age  installment_plans  \\\n",
       "count  1000.000000     ...       1000.000000  1000.000000        1000.000000   \n",
       "mean    101.145000     ...        122.358000    35.546000         142.675000   \n",
       "std       0.477706     ...          1.050209    11.375469           0.705601   \n",
       "min     101.000000     ...        121.000000    19.000000         141.000000   \n",
       "25%     101.000000     ...        121.000000    27.000000         143.000000   \n",
       "50%     101.000000     ...        122.000000    33.000000         143.000000   \n",
       "75%     101.000000     ...        123.000000    42.000000         143.000000   \n",
       "max     103.000000     ...        124.000000    75.000000         143.000000   \n",
       "\n",
       "           housing  existing_credits          job  liable_people    telephone  \\\n",
       "count  1000.000000       1000.000000  1000.000000    1000.000000  1000.000000   \n",
       "mean    151.929000          1.407000   172.904000       1.155000   191.404000   \n",
       "std       0.531264          0.577654     0.653614       0.362086     0.490943   \n",
       "min     151.000000          1.000000   171.000000       1.000000   191.000000   \n",
       "25%     152.000000          1.000000   173.000000       1.000000   191.000000   \n",
       "50%     152.000000          1.000000   173.000000       1.000000   191.000000   \n",
       "75%     152.000000          2.000000   173.000000       1.000000   192.000000   \n",
       "max     153.000000          4.000000   174.000000       2.000000   192.000000   \n",
       "\n",
       "       foreign_worker       target  \n",
       "count     1000.000000  1000.000000  \n",
       "mean       201.037000     1.300000  \n",
       "std          0.188856     0.458487  \n",
       "min        201.000000     1.000000  \n",
       "25%        201.000000     1.000000  \n",
       "50%        201.000000     1.000000  \n",
       "75%        201.000000     2.000000  \n",
       "max        202.000000     2.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "O’Dea, et. al. used the thermometer coding scheme in order to represent the attribute values. For example for the status attribute, the values less-200DM were coded as {001}, over-200DM was coded as {011}, and no-account was coded as {111}. Also, continuous data, such as duration and credit_amount were binned (or bucketed)  in order to aggregate the data into a more useful form for the model to use. Table 2 of Dea, shows the binary representation used to represent the inputs to the neural network (25 inputs in all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Using information theory, as outlined in Provost and Fawcett (2013). We calculated the information gained for each of the attributes. As an example we will display the calculations for the attribute `status`.\n",
    "\n",
    "First, based on the data set, the overall probabilities of good credit or bad credit can be calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of good credit = 0.7\n",
      "Probability of bad credit = 0.3\n"
     ]
    }
   ],
   "source": [
    "p_parent_good = german.where(german.target==1).dropna().shape[0]/german.shape[0]\n",
    "print(\"Probability of good credit = {}\".format(p_parent_good))\n",
    "p_parent_bad = german.where(german.target==2).dropna().shape[0]/german.shape[0]\n",
    "print(\"Probability of bad credit = {}\".format(p_parent_bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total entropy of the data set is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Entropy = 0.881290899231\n"
     ]
    }
   ],
   "source": [
    "parent_entropy = - (p_parent_good * log(p_parent_good, 2) + p_parent_bad * log(p_parent_bad, 2))\n",
    "print(\"Parent Entropy = {}\".format(parent_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand how informative the attribute is we need to calculate the information gained. This is done by calculating how much the attribute reduces the entropy of the segmentations created by splitting the data set along the values of the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of value 11: 0.274\n",
      "Probability for value 11, to have good credit: 0.507299270073\n",
      "Probability for value 11, to have bad credit: 0.492700729927\n",
      "Entropy of child with value 11: 0.999846262849\n",
      "--------------------------------------------------------\n",
      "Probability of value 12: 0.269\n",
      "Probability for value 12, to have good credit: 0.609665427509\n",
      "Probability for value 12, to have bad credit: 0.390334572491\n",
      "Entropy of child with value 12: 0.965015120503\n",
      "--------------------------------------------------------\n",
      "Probability of value 13: 0.063\n",
      "Probability for value 13, to have good credit: 0.777777777778\n",
      "Probability for value 13, to have bad credit: 0.222222222222\n",
      "Entropy of child with value 13: 0.764204506509\n",
      "--------------------------------------------------------\n",
      "Probability of value 14: 0.394\n",
      "Probability for value 14, to have good credit: 0.883248730964\n",
      "Probability for value 14, to have bad credit: 0.116751269036\n",
      "Entropy of child with value 14: 0.519949823177\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "feature_values_for_status = [11,12,13,14]\n",
    "\n",
    "status_value_series = {}\n",
    "#Split the data set along the values of the attributes.\n",
    "for value in feature_values_for_status:\n",
    "    status_value_series[value] = german.where(german['status']==value).dropna().target\n",
    "\n",
    "IG_children = 0\n",
    "for key, series in status_value_series.iteritems():\n",
    "    p_status_value = series.shape[0] / german.shape[0]\n",
    "    p_series_good = series.where(series==1).dropna().shape[0] / series.shape[0]\n",
    "    p_series_bad = series.where(series==2).dropna().shape[0] / series.shape[0]\n",
    "    entropy_child = -(p_series_good * log(p_series_good, 2) + p_series_bad * log(p_series_bad, 2))\n",
    "    IG_children = IG_children + (p_status_value * entropy_child)\n",
    "    print(\"Probability of value {}: {}\".format(key, p_status_value))\n",
    "    print(\"Probability for value {}, to have good credit: {}\".format(key, p_series_good))\n",
    "    print(\"Probability for value {}, to have bad credit: {}\".format(key, p_series_bad))\n",
    "    print(\"Entropy of child with value {}: {}\".format(key, entropy_child))\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the sum of the products of the children entropies and probabilities and subtract it from the entropy of the parent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for attribute status: 0.0947388415526\n"
     ]
    }
   ],
   "source": [
    "IG = parent_entropy - IG_children\n",
    "print(\"Information Gain for attribute status: {}\".format(IG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning Attributes\n",
    "\n",
    "It is useful to bin or basket together numerical data. You loose data doing this but the outcome is more useful for the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "german.duration = pd.cut(german.duration, bins=4, labels=False, include_lowest=True)\n",
    "german.credit_amount = pd.cut(german.credit_amount, bins=4, labels=False, include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Noted as most relevant attributes from the paper are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "german2 = german[['duration', 'credit_history', 'credit_amount', 'savings', \n",
    "                  'status', 'housing', 'foreign_worker', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings</th>\n",
       "      <th>status</th>\n",
       "      <th>housing</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>11</td>\n",
       "      <td>152</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>12</td>\n",
       "      <td>152</td>\n",
       "      <td>201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>14</td>\n",
       "      <td>152</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>153</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>153</td>\n",
       "      <td>201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  credit_history  credit_amount  savings  status  housing  \\\n",
       "0         0              34              0       65      11      152   \n",
       "1         2              32              1       61      12      152   \n",
       "2         0              34              0       61      14      152   \n",
       "3         2              32              1       61      11      153   \n",
       "4         1              33              1       61      11      153   \n",
       "\n",
       "   foreign_worker  target  \n",
       "0             201       1  \n",
       "1             201       2  \n",
       "2             201       1  \n",
       "3             201       1  \n",
       "4             201       2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chop up the data (training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = german2.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = german['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_labels, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'status', u'duration', u'credit_history', u'purpose', u'credit_amount',\n",
       "       u'savings', u'employment_duration', u'installment_rate',\n",
       "       u'personal_status', u'debtors', u'residence', u'property', u'age',\n",
       "       u'installment_plans', u'housing', u'existing_credits', u'job',\n",
       "       u'liable_people', u'telephone', u'foreign_worker', u'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based on Neural Networks\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "We chose to use a high level api provided by TensorFlow. TensorFlow has a number of estimator API’s, which allows a user to tap into pre-built models. We happened to choose the DNN Classifier (dense neural network). We also ran the data through TF’s Linear Classifier, but decided to go with the dense neural network because it produced better results. The DNN Classifier is also an ideal model for binary classification problems - in our case we needed to predict 1 or 2 (Good vs Bad).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign feature columns for TensorFlow (categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "for key in german2.drop('target', axis=1).columns:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['status', 'credit_history', 'purpose', 'savings', 'employment_duration', 'personal_status', 'debtors', 'property', 'installment_plans', 'housing', 'job', 'telephone', 'foreign_worker']\n",
      "['duration', 'credit_amount', 'installment_rate', 'residence', 'age', 'existing_credits', 'liable_people']\n"
     ]
    }
   ],
   "source": [
    "for attribute in attribute_dict.values():\n",
    "    print(attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "First, the user would define the various features. The main part of defining the features in TF is setting the type of date, for instance is the data categorical or numeric. In our case all the categorical attributes were translated into numbers, so we set each feature to numeric.\n",
    "\n",
    "Second, we defined the input function. This packages the data in the form of a Pandas dataframe and inserts it into the TensorFlow model. Within this input function you can set the number of epochs, batch size etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_func = tf.estimator.inputs.pandas_input_fn(x=X_train,y=y_train,batch_size=10,num_epochs=1000,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we defined the actual model. In this function the user would set the number of neurons as well as the number of hidden layers. The model also comes equipped with gradient descent optimizers - the default is the Adagrad Algorithm which is what we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp5wTR4y\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f860158d3d0>, '_model_dir': '/tmp/tmp5wTR4y', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "dnn_model = tf.estimator.DNNClassifier(hidden_units=[10,10],n_classes=4, feature_columns=feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we ran the model by using the train function on the model. This function took in the argument of how many steps the model should run for and your input function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jbennett/.local/lib/python2.7/site-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py:62: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/jbennett/.local/lib/python2.7/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /home/jbennett/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp5wTR4y/model.ckpt.\n",
      "INFO:tensorflow:loss = 18.705767, step = 1\n",
      "INFO:tensorflow:global_step/sec: 299.016\n",
      "INFO:tensorflow:loss = 6.2550282, step = 101 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 491.524\n",
      "INFO:tensorflow:loss = 5.184935, step = 201 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 400.993\n",
      "INFO:tensorflow:loss = 5.175823, step = 301 (0.250 sec)\n",
      "INFO:tensorflow:global_step/sec: 408.634\n",
      "INFO:tensorflow:loss = 5.267221, step = 401 (0.244 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 500 into /tmp/tmp5wTR4y/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 6.985504.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x7f8610c406d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.train(input_fn=input_func,steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input_func = tf.estimator.inputs.pandas_input_fn(x=X_test,y=y_test,batch_size=10,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensorflow api comes equipped with a evaluation method in which you can input your testing set and compare against the actual targets. In addition to this, we created a scoring system where a penalty would be applied for wrong predictions. 5 points would be allotted if the model predicted a 1 but the actual was 2. 1 point was was allotted if it had predicted a 2 rather than the actual value 1. 0 points for correct answers. In short we looked for a model that maximized the accuracy percentage and minimized the penalty score.\n",
    "\n",
    "A python script was developed to take in a range of parameters (ie. layers and neurons) and test the various models in bulk. We took the averages and graphed the various outputs using excel. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-02-16:26:56\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp5wTR4y/model.ckpt-500\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-02-16:26:56\n",
      "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.655, average_loss = 0.64779013, global_step = 500, loss = 6.4779015\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: /tmp/tmp5wTR4y/model.ckpt-500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.655,\n",
       " 'average_loss': 0.64779013,\n",
       " 'global_step': 500,\n",
       " 'loss': 6.4779015}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.evaluate(eval_input_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitehorn, M. (2006). \"The parable of the beer and diapers\", The Register. Retrieved from: https://www.theregister.co.uk/2006/08/15/beer_diapers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provost, F. & Fawcett, T. (2013). \"Data Science for Business\". O'Reilly. Sebastopol, CA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O'Dea, P., Griffith, J., O'Riordan, C. \"Combining Feature Selection and Neural Networks for Solving Classification Problems\". National University of Ireland. Galway, Galway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
