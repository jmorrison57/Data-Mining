{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German Credit Analysis, Using DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of the project\n",
    "\n",
    "The purpose of this project is to replicate the findings in \"Combining Feature Selection and Neural Networks for Solving Classification Problems,\" O'Dea, P., Griffith, J., O'Riordan, C. This paper used feature selection and Neural Networks to solve a classification problem using the German Credit data set.\n",
    "\n",
    "This approach took the form of two phases: The first phase was to use Information Theory to determine the attributes of the German credit data set that are the most important in classifiying the data record as good credit or bad credit. The second phase was to process the data and transform it into a form usable by the neural network model, train the model using 97.5% of the German Credit data set and finally test the model using the remaining 2.5% of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Mining Methods and their Applications\n",
    "\n",
    "There are many different data mining methods used to solve many different problems. Many of these methods are outlined in Provost & Fawcett pages 20-23:\n",
    "\n",
    "- Data mining is deployed in many different \"classification and association rules, and to item-set recognition and sequential pattern recoginition problems\" (O'Dea, p. 1). Many of these methods are outlined in Provost & Fawcett pages 20-23.\n",
    "\n",
    "\n",
    "- When deploying classification methods, the key objective is to predict which class an item belong to based on a set of attributes (Provost & Fawcett, p. 20). Applications of classification involve prediciting customer churn, or in classifying what class an item belongs to. Examples of methods involved in classifications are Neural Networks, score cards, and decision trees.\n",
    "\n",
    "\n",
    "- Regression, or \"value estimation\" (Provost & Fawcett, p. 21) is a technique that attempts to predict the numerical value based on past data. The technique uses a linear mathematical model (often using the least squares approach), that can predict the dependent variable from an independent variable. Applications of regression models include predicting the temperature on a given day based on previous days (or seasonal data).\n",
    "\n",
    "\n",
    "- Similarity matching used known data about items and attempts to identify similar items. Applications of similarity matching include finding customers that are similar to your best customer.\n",
    "\n",
    "\n",
    "- Clustering methods is an unsupervisied (no target attribute), that attempts to cluster or group like items together. Applications of clustering include finding segments in groups of customers.\n",
    "\n",
    "\n",
    "- Co-occurrence grouping is a data mining method that attempts to associate items based on their transactions. It is also called assiaction rule discovery or market-basket analysis. An application of co-occurrence grouping is to suggest products to customers based on the items they have in their shopping cart. A famous example of co-occurrence grouping is the diapers and beer association (Whitehorn, 2006).\n",
    "\n",
    "\n",
    "- Profiling is a data mining method that tries to describe the behavior of individuals. It is most useful in \"fraud detection and monitoring intrusion detection\" (Provost & Fawcett, p. 22).\n",
    "\n",
    "\n",
    "- Link prediction attempt to determine whether or not a link between items should exist based on associated other links. A common applicaiton of link prediction is friend sugestions in Facebook.\n",
    "\n",
    "\n",
    "- Data reduction is a data mining method that attempts to reduce the complexity and volume of a large data set to a more manageable size by pruning the less important information. Data reduction often results in data loss, but at the gain of performance and easier understanding of the data set involved.\n",
    "\n",
    "\n",
    "- Causal modeling tries to find links between events and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techniques and Approaches\n",
    "\n",
    "The techniques used in this model include data segmentation, data reduction, and classification. The first phase of the project is to determine the most important attributes to use when training and testing the model, and reduce the data set to those attributes. This is done using Information Theory.\n",
    "\n",
    "The second phase is to process the reduced data set into a form that can be easily used by the model. This is done by binning the data sets for numerical data into clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Industry Standard Process for Data Mining (CRISP-DM)\n",
    "\n",
    "We need a process for data mining in order to have a reasonable amount of “consistency, repeatability, and objectiveness” (Provost, 2013, p. 27) of the outcomes. The CRISP-DM method is one of the most popular data mining processes used in the industry. The CRISP-DM process is an iterative process. Each iteration helps to inform about the data.\n",
    "\n",
    "\n",
    "## Business Understanding\n",
    "\n",
    "The business understanding defines the “problem to be solved” (p. 28). The problem here is a feature selection and classification problem: Using the German dataset we need to predict if an applicant is a “good or bad credit risk” (O’Dae, Griffith, O’Riordan; p. 6).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from math import log\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "The data is composed of twenty attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "german = pd.read_csv('MGMT635_GermanCreditData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings</th>\n",
       "      <th>employment_duration</th>\n",
       "      <th>installment_rate</th>\n",
       "      <th>personal_status</th>\n",
       "      <th>debtors</th>\n",
       "      <th>...</th>\n",
       "      <th>property</th>\n",
       "      <th>age</th>\n",
       "      <th>installment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>liable_people</th>\n",
       "      <th>telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>43</td>\n",
       "      <td>1169</td>\n",
       "      <td>65</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>93</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>67</td>\n",
       "      <td>143</td>\n",
       "      <td>152</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>48</td>\n",
       "      <td>32</td>\n",
       "      <td>43</td>\n",
       "      <td>5951</td>\n",
       "      <td>61</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>22</td>\n",
       "      <td>143</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "      <td>191</td>\n",
       "      <td>201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>46</td>\n",
       "      <td>2096</td>\n",
       "      <td>61</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>49</td>\n",
       "      <td>143</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>7882</td>\n",
       "      <td>61</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "      <td>103</td>\n",
       "      <td>...</td>\n",
       "      <td>122</td>\n",
       "      <td>45</td>\n",
       "      <td>143</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>4870</td>\n",
       "      <td>61</td>\n",
       "      <td>73</td>\n",
       "      <td>3</td>\n",
       "      <td>93</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>124</td>\n",
       "      <td>53</td>\n",
       "      <td>143</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>173</td>\n",
       "      <td>2</td>\n",
       "      <td>191</td>\n",
       "      <td>201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   status  duration  credit_history  purpose  credit_amount  savings  \\\n",
       "0      11         6              34       43           1169       65   \n",
       "1      12        48              32       43           5951       61   \n",
       "2      14        12              34       46           2096       61   \n",
       "3      11        42              32       42           7882       61   \n",
       "4      11        24              33       40           4870       61   \n",
       "\n",
       "   employment_duration  installment_rate  personal_status  debtors   ...    \\\n",
       "0                   75                 4               93      101   ...     \n",
       "1                   73                 2               92      101   ...     \n",
       "2                   74                 2               93      101   ...     \n",
       "3                   74                 2               93      103   ...     \n",
       "4                   73                 3               93      101   ...     \n",
       "\n",
       "   property  age  installment_plans  housing  existing_credits  job  \\\n",
       "0       121   67                143      152                 2  173   \n",
       "1       121   22                143      152                 1  173   \n",
       "2       121   49                143      152                 1  172   \n",
       "3       122   45                143      153                 1  173   \n",
       "4       124   53                143      153                 2  173   \n",
       "\n",
       "   liable_people  telephone  foreign_worker  target  \n",
       "0              1        192             201       1  \n",
       "1              1        191             201       2  \n",
       "2              2        191             201       1  \n",
       "3              2        191             201       1  \n",
       "4              2        191             201       2  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of attributes: Categorical which are in dictionary 1, and numerical which is in dictionary 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attribute_dict = {1:['status', 'credit_history', 'purpose', 'savings',\n",
    "                    'employment_duration', 'personal_status', 'debtors','property', 'installment_plans', 'housing', \n",
    "                    'job', 'telephone', 'foreign_worker'], \n",
    "                  2:['duration', 'credit_amount', 'installment_rate', 'residence', 'age', 'existing_credits',\n",
    "                     'liable_people']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to get insights into the data set is to get some basic statistics. Below is a display of some basic statisitcal information which will tell us things such as mean, standard deviation, minimum and maximum values. For categorical data, the mode would be a useful statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings</th>\n",
       "      <th>employment_duration</th>\n",
       "      <th>installment_rate</th>\n",
       "      <th>personal_status</th>\n",
       "      <th>debtors</th>\n",
       "      <th>...</th>\n",
       "      <th>property</th>\n",
       "      <th>age</th>\n",
       "      <th>installment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>liable_people</th>\n",
       "      <th>telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.577000</td>\n",
       "      <td>20.903000</td>\n",
       "      <td>32.54500</td>\n",
       "      <td>47.148000</td>\n",
       "      <td>3271.258000</td>\n",
       "      <td>62.105000</td>\n",
       "      <td>73.384000</td>\n",
       "      <td>2.973000</td>\n",
       "      <td>92.68200</td>\n",
       "      <td>101.145000</td>\n",
       "      <td>...</td>\n",
       "      <td>122.358000</td>\n",
       "      <td>35.546000</td>\n",
       "      <td>142.675000</td>\n",
       "      <td>151.929000</td>\n",
       "      <td>1.407000</td>\n",
       "      <td>172.904000</td>\n",
       "      <td>1.155000</td>\n",
       "      <td>191.404000</td>\n",
       "      <td>201.037000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.257638</td>\n",
       "      <td>12.058814</td>\n",
       "      <td>1.08312</td>\n",
       "      <td>40.095333</td>\n",
       "      <td>2822.736876</td>\n",
       "      <td>1.580023</td>\n",
       "      <td>1.208306</td>\n",
       "      <td>1.118715</td>\n",
       "      <td>0.70808</td>\n",
       "      <td>0.477706</td>\n",
       "      <td>...</td>\n",
       "      <td>1.050209</td>\n",
       "      <td>11.375469</td>\n",
       "      <td>0.705601</td>\n",
       "      <td>0.531264</td>\n",
       "      <td>0.577654</td>\n",
       "      <td>0.653614</td>\n",
       "      <td>0.362086</td>\n",
       "      <td>0.490943</td>\n",
       "      <td>0.188856</td>\n",
       "      <td>0.458487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>91.00000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>151.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>171.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1365.500000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>92.00000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>32.00000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2319.500000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>93.00000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>3972.250000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>93.00000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>152.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>34.00000</td>\n",
       "      <td>410.000000</td>\n",
       "      <td>18424.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>94.00000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            status     duration  credit_history      purpose  credit_amount  \\\n",
       "count  1000.000000  1000.000000      1000.00000  1000.000000    1000.000000   \n",
       "mean     12.577000    20.903000        32.54500    47.148000    3271.258000   \n",
       "std       1.257638    12.058814         1.08312    40.095333    2822.736876   \n",
       "min      11.000000     4.000000        30.00000    40.000000     250.000000   \n",
       "25%      11.000000    12.000000        32.00000    41.000000    1365.500000   \n",
       "50%      12.000000    18.000000        32.00000    42.000000    2319.500000   \n",
       "75%      14.000000    24.000000        34.00000    43.000000    3972.250000   \n",
       "max      14.000000    72.000000        34.00000   410.000000   18424.000000   \n",
       "\n",
       "           savings  employment_duration  installment_rate  personal_status  \\\n",
       "count  1000.000000          1000.000000       1000.000000       1000.00000   \n",
       "mean     62.105000            73.384000          2.973000         92.68200   \n",
       "std       1.580023             1.208306          1.118715          0.70808   \n",
       "min      61.000000            71.000000          1.000000         91.00000   \n",
       "25%      61.000000            73.000000          2.000000         92.00000   \n",
       "50%      61.000000            73.000000          3.000000         93.00000   \n",
       "75%      63.000000            75.000000          4.000000         93.00000   \n",
       "max      65.000000            75.000000          4.000000         94.00000   \n",
       "\n",
       "           debtors     ...          property          age  installment_plans  \\\n",
       "count  1000.000000     ...       1000.000000  1000.000000        1000.000000   \n",
       "mean    101.145000     ...        122.358000    35.546000         142.675000   \n",
       "std       0.477706     ...          1.050209    11.375469           0.705601   \n",
       "min     101.000000     ...        121.000000    19.000000         141.000000   \n",
       "25%     101.000000     ...        121.000000    27.000000         143.000000   \n",
       "50%     101.000000     ...        122.000000    33.000000         143.000000   \n",
       "75%     101.000000     ...        123.000000    42.000000         143.000000   \n",
       "max     103.000000     ...        124.000000    75.000000         143.000000   \n",
       "\n",
       "           housing  existing_credits          job  liable_people    telephone  \\\n",
       "count  1000.000000       1000.000000  1000.000000    1000.000000  1000.000000   \n",
       "mean    151.929000          1.407000   172.904000       1.155000   191.404000   \n",
       "std       0.531264          0.577654     0.653614       0.362086     0.490943   \n",
       "min     151.000000          1.000000   171.000000       1.000000   191.000000   \n",
       "25%     152.000000          1.000000   173.000000       1.000000   191.000000   \n",
       "50%     152.000000          1.000000   173.000000       1.000000   191.000000   \n",
       "75%     152.000000          2.000000   173.000000       1.000000   192.000000   \n",
       "max     153.000000          4.000000   174.000000       2.000000   192.000000   \n",
       "\n",
       "       foreign_worker       target  \n",
       "count     1000.000000  1000.000000  \n",
       "mean       201.037000     1.300000  \n",
       "std          0.188856     0.458487  \n",
       "min        201.000000     1.000000  \n",
       "25%        201.000000     1.000000  \n",
       "50%        201.000000     1.000000  \n",
       "75%        201.000000     2.000000  \n",
       "max        202.000000     2.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Using entropy and information gain, seven attributes were selected as the most useful in determining the credit worthiness of a candidate based on information gain. The attributes selected were: the status of existing checking account, the credit duration in months, credit history, credit amount, savings accounts/bonds, housing (rent, own, for free), and whether or not the person is a foreign worker.\n",
    "\n",
    "O’Dea, et. al. used the thermometer coding scheme in order to represent the attribute values. For example for the status attribute, the values less-200DM were coded as {001}, over-200DM was coded as {011}, and no-account was coded as {111}. Also, continuous data, such as duration and credit_amount were binned (or bucketed) in order to aggregate the data into a more useful form for the model to use. Table 2 of Dea, shows the binary representation used to represent the inputs to the neural network (25 inputs in all).\n",
    "\n",
    "In our project we demonstrate how feature selection is used to discover interesting attributes to use for the model, how binning can transform data from numerical to categorical, and finally how the data can be shuffled and split into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Using information theory, as outlined in Provost and Fawcett (2013). We calculated the information gained for each of the attributes. As an example we will display the calculations for the attribute status.\n",
    "\n",
    "First, based on the data set, the overall probabilities of good credit or bad credit can be calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of good credit = 0.7\n",
      "Probability of bad credit = 0.3\n"
     ]
    }
   ],
   "source": [
    "p_parent_good = german.where(german.target==1).dropna().shape[0]/german.shape[0]\n",
    "print(\"Probability of good credit = {}\".format(p_parent_good))\n",
    "p_parent_bad = german.where(german.target==2).dropna().shape[0]/german.shape[0]\n",
    "print(\"Probability of bad credit = {}\".format(p_parent_bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total entropy of the data set is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Entropy = 0.8812908992306927\n"
     ]
    }
   ],
   "source": [
    "parent_entropy = - (p_parent_good * log(p_parent_good, 2) + p_parent_bad * log(p_parent_bad, 2))\n",
    "print(\"Parent Entropy = {}\".format(parent_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand how informative the attribute is we need to calculate the information gained. This is done by calculating how much the attribute reduces the entropy of the segmentations created by splitting the data set along the values of the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of value 11: 0.274\n",
      "Probability for value 11, to have good credit: 0.5072992700729927\n",
      "Probability for value 11, to have bad credit: 0.4927007299270073\n",
      "Entropy of child with value 11: 0.9998462628494693\n",
      "--------------------------------------------------------\n",
      "Probability of value 12: 0.269\n",
      "Probability for value 12, to have good credit: 0.6096654275092936\n",
      "Probability for value 12, to have bad credit: 0.3903345724907063\n",
      "Entropy of child with value 12: 0.9650151205034324\n",
      "--------------------------------------------------------\n",
      "Probability of value 13: 0.063\n",
      "Probability for value 13, to have good credit: 0.7777777777777778\n",
      "Probability for value 13, to have bad credit: 0.2222222222222222\n",
      "Entropy of child with value 13: 0.7642045065086203\n",
      "--------------------------------------------------------\n",
      "Probability of value 14: 0.394\n",
      "Probability for value 14, to have good credit: 0.883248730964467\n",
      "Probability for value 14, to have bad credit: 0.116751269035533\n",
      "Entropy of child with value 14: 0.5199498231772391\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "feature_values_for_status = [11,12,13,14]\n",
    "\n",
    "status_value_series = {}\n",
    "#Split the data set along the values of the attributes.\n",
    "for value in feature_values_for_status:\n",
    "    status_value_series[value] = german.where(german['status']==value).dropna().target\n",
    "\n",
    "IG_children = 0\n",
    "for key, series in status_value_series.items():\n",
    "    p_status_value = series.shape[0] / german.shape[0]\n",
    "    p_series_good = series.where(series==1).dropna().shape[0] / series.shape[0]\n",
    "    p_series_bad = series.where(series==2).dropna().shape[0] / series.shape[0]\n",
    "    entropy_child = -(p_series_good * log(p_series_good, 2) + p_series_bad * log(p_series_bad, 2))\n",
    "    IG_children = IG_children + (p_status_value * entropy_child)\n",
    "    print(\"Probability of value {}: {}\".format(key, p_status_value))\n",
    "    print(\"Probability for value {}, to have good credit: {}\".format(key, p_series_good))\n",
    "    print(\"Probability for value {}, to have bad credit: {}\".format(key, p_series_bad))\n",
    "    print(\"Entropy of child with value {}: {}\".format(key, entropy_child))\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the sum of the products of the children entropies and probabilities and subtract it from the entropy of the parent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for attribute status: 0.09473884155263945\n"
     ]
    }
   ],
   "source": [
    "IG = parent_entropy - IG_children\n",
    "print(\"Information Gain for attribute status: {}\".format(IG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binning Attributes\n",
    "\n",
    "It is useful to bin or basket together numerical data. You loose data doing this but the outcome is more useful for the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german.duration = pd.cut(german.duration, bins=4, labels=False, include_lowest=True)\n",
    "german.credit_amount = pd.cut(german.credit_amount, bins=4, labels=False, include_lowest=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Noted as most relevant attributes from the paper are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "german2 = german[['duration', 'credit_history', 'credit_amount', 'savings', \n",
    "                  'status', 'housing', 'foreign_worker', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings</th>\n",
       "      <th>status</th>\n",
       "      <th>housing</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>11</td>\n",
       "      <td>152</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>12</td>\n",
       "      <td>152</td>\n",
       "      <td>201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>14</td>\n",
       "      <td>152</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>153</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>153</td>\n",
       "      <td>201</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  credit_history  credit_amount  savings  status  housing  \\\n",
       "0         0              34              0       65      11      152   \n",
       "1         2              32              1       61      12      152   \n",
       "2         0              34              0       61      14      152   \n",
       "3         2              32              1       61      11      153   \n",
       "4         1              33              1       61      11      153   \n",
       "\n",
       "   foreign_worker  target  \n",
       "0             201       1  \n",
       "1             201       2  \n",
       "2             201       1  \n",
       "3             201       1  \n",
       "4             201       2  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german2.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up the data into a training/testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = german2.drop('target',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_labels = german['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_labels, test_size=0.025, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_train = X_train.min(axis=0)\n",
    "max_train = X_train.max(axis=0)\n",
    "\n",
    "range_train = (X_train - min_train).max(axis=0)\n",
    "range_train_new = max_train-min_train\n",
    "\n",
    "X_train_scaled = (X_train - min_train)/range_train\n",
    "X_test_scaled = (X_test - min_train)/range_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['status', 'duration', 'credit_history', 'purpose', 'credit_amount',\n",
       "       'savings', 'employment_duration', 'installment_rate', 'personal_status',\n",
       "       'debtors', 'residence', 'property', 'age', 'installment_plans',\n",
       "       'housing', 'existing_credits', 'job', 'liable_people', 'telephone',\n",
       "       'foreign_worker', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based on Neural Networks\n",
    "\n",
    "## Model Selection\n",
    "\n",
    "We chose to use a high level api provided by TensorFlow. TensorFlow has a number of estimator API’s, which allows a user to tap into pre-built models. We happened to choose the DNN Classifier (dense neural network). We also ran the data through TF’s Linear Classifier, but decided to go with the dense neural network because it produced better results. The DNN Classifier is also an ideal model for binary classification problems - in our case we needed to predict 1 or 2 (Good vs Bad).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "\n",
    "First, the user would define the various features. The main part of defining the features in TF is setting the type of date, for instance is the data categorical or numeric. In our case all the categorical attributes were translated into numbers, so we set each feature to numeric.\n",
    "\n",
    "Second, we defined the input function. This packages the data in the form of a Pandas dataframe and inserts it into the TensorFlow model. Within this input function you can set the number of epochs, batch size etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "for key in german2.drop('target', axis=1).columns:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['status', 'credit_history', 'purpose', 'savings', 'employment_duration', 'personal_status', 'debtors', 'property', 'installment_plans', 'housing', 'job', 'telephone', 'foreign_worker']\n",
      "['duration', 'credit_amount', 'installment_rate', 'residence', 'age', 'existing_credits', 'liable_people']\n"
     ]
    }
   ],
   "source": [
    "for attribute in attribute_dict.values():\n",
    "    print(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_func = tf.estimator.inputs.pandas_input_fn(x=X_train_scaled,y=y_train,batch_size=10,num_epochs=1000,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we defined the actual model. In this function the user would set the number of neurons as well as the number of hidden layers. The model also comes equipped with gradient descent optimizers - the default is the Adagrad Algorithm which is what we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\JMORR_~1\\AppData\\Local\\Temp\\tmpnatmvjf5\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': 1, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_save_checkpoints_steps': None, '_session_config': None, '_save_checkpoints_secs': 600, '_model_dir': 'C:\\\\Users\\\\JMORR_~1\\\\AppData\\\\Local\\\\Temp\\\\tmpnatmvjf5', '_save_summary_steps': 100, '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    }
   ],
   "source": [
    "dnn_model = tf.estimator.DNNClassifier(hidden_units=[8,8,8],n_classes=4, feature_columns=feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we ran the model by using the train function on the model. This function took in the argument of how many steps the model should run for and your input function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\JMORR_~1\\AppData\\Local\\Temp\\tmpnatmvjf5\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 14.6734\n",
      "INFO:tensorflow:global_step/sec: 145.459\n",
      "INFO:tensorflow:step = 101, loss = 5.26723 (0.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 147.98\n",
      "INFO:tensorflow:step = 201, loss = 5.22755 (0.684 sec)\n",
      "INFO:tensorflow:global_step/sec: 132.923\n",
      "INFO:tensorflow:step = 301, loss = 5.94099 (0.745 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.434\n",
      "INFO:tensorflow:step = 401, loss = 5.00192 (0.719 sec)\n",
      "INFO:tensorflow:global_step/sec: 116.307\n",
      "INFO:tensorflow:step = 501, loss = 4.88976 (0.850 sec)\n",
      "INFO:tensorflow:global_step/sec: 113.86\n",
      "INFO:tensorflow:step = 601, loss = 4.68564 (0.874 sec)\n",
      "INFO:tensorflow:global_step/sec: 135.305\n",
      "INFO:tensorflow:step = 701, loss = 2.6906 (0.749 sec)\n",
      "INFO:tensorflow:global_step/sec: 111.445\n",
      "INFO:tensorflow:step = 801, loss = 3.50019 (0.889 sec)\n",
      "INFO:tensorflow:global_step/sec: 140.443\n",
      "INFO:tensorflow:step = 901, loss = 4.17659 (0.724 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\JMORR_~1\\AppData\\Local\\Temp\\tmpnatmvjf5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.68904.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x362d6bc4a8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.train(input_fn=input_func,steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluations\n",
    "\n",
    "The tensorflow api comes equipped with a evaluation method. Here you can input your testing set and compare the results against the actual targets. In addition to this, we created a scoring system where a penalty would be applied for wrong predictions. 5 points would be allotted if the model predicted a 1 but the actual was a 2. 1 point was was allotted if it had predicted a 2 buth the actual was a 1. 0 points for correct answers. In short we looked for a model that maximized the accuracy percentage and minimized the penalty score.\n",
    "\n",
    "To stream line the testing process we developed a script that allowed a user to input a range of parameters. The user would also input how many times each type of model should run. For our testing we had the model run 5 times each and then we averaged the results. The layers we tested ranged from 1 to 5, and neurons ranged from 7 to 10. The output of these tests can be found in the \"Testing Results\" folder. \n",
    "\n",
    "The optimal model proved to have 8 neurons and 3 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_input_func = tf.estimator.inputs.pandas_input_fn(x=X_test_scaled,y=y_test,batch_size=10,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-03-03-00:00:46\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\JMORR_~1\\AppData\\Local\\Temp\\tmpnatmvjf5\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-03-00:00:48\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.8, average_loss = 0.448037, global_step = 1000, loss = 3.73364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.80000001,\n",
       " 'average_loss': 0.44803688,\n",
       " 'global_step': 1000,\n",
       " 'loss': 3.7336407}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_model.evaluate(eval_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-03-03-00:00:54\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\JMORR_~1\\AppData\\Local\\Temp\\tmpnatmvjf5\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-03-00:00:56\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.8, average_loss = 0.448037, global_step = 1000, loss = 3.73364\n"
     ]
    }
   ],
   "source": [
    "evaluation = dnn_model.evaluate(eval_input_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_eval = pd.DataFrame([{\"Accuracy\": evaluation['accuracy'],\"Loss\":evaluation[\"loss\"],\"Average Loss\": evaluation[\"average_loss\"]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\JMORR_~1\\AppData\\Local\\Temp\\tmpnatmvjf5\\model.ckpt-1000\n"
     ]
    }
   ],
   "source": [
    "predictions = list(dnn_model.predict(eval_input_func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_preds = []\n",
    "for pred in predictions:\n",
    "    final_preds.append(pred['class_ids'][0])\n",
    "    \n",
    "test_targets = []\n",
    "for y in y_test:\n",
    "    test_targets.append(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalty Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"Predicted Result\": final_preds,\"Actual Result\":test_targets})\n",
    "score = 0\n",
    "index = 0\n",
    "\n",
    "for x in results['Actual Result']:\n",
    "    if x == 2 and results[\"Predicted Result\"][index] == 1:\n",
    "        score = score + 5\n",
    "    if x == 1 and results[\"Predicted Result\"][index] == 2:\n",
    "        score = score + 1\n",
    "    else:\n",
    "        score = score + 0\n",
    "    index += 1\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_actu = pd.Series(test_targets, name='Actual')\n",
    "y_pred = pd.Series(final_preds, name='Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   1  2  All\n",
       "Actual               \n",
       "1          17  1   18\n",
       "2           4  3    7\n",
       "All        21  4   25"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion = pd.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "df_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('Results.xlsx') as writer:\n",
    "    results.to_excel(writer, sheet_name='Output')\n",
    "    tf_eval.to_excel(writer, sheet_name='Evaluation')\n",
    "    df_confusion.to_excel(writer, sheet_name='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "file = \"Results.xlsx\"\n",
    "os.startfile(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "This model could be deployed to loan officers who could use it to conduct preliminary screenings of applicants. The model could be embedded in a web application. The bank representative could input a user’s information and it would output if the applicant was a good or bad candidate. The rep could then use this information to present appropriate options for the potential borrower. We would not suggest using this model to be the end all solution for determining who gets a loan, it should mainly be used as an additional tool.\n",
    "\n",
    "As data is acquired from their lending base it can be used to retrain the model. To sufficiently do this a data pipeline could be constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The chart below shows the results of the models with 8 neurons. As you can see the model with 3 hidden layers performed the best. This model maximized the average accuracy yielding 83.2% and minimized the average penalty score of 14.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"accuracy.png\"><img src=\"accuracy_graph.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We successfully replicated the findings in \"Combining Feature Selection and Neural Networks for Solving Classification Problems,\", and slightly exceeded their performance of their model. One caveat might be that their hold out set was slightly larger which may have contributed to their lower classification accuracy of 74.25% compared to our 83.2%. \n",
    "\n",
    "Within this project we demonstrated feature selection, data preprocessing, parameter tuning, testing strategies, and a deployment roadmap. This approach justifies why neural networks are a good tool for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitehorn, M. (2006). \"The parable of the beer and diapers\", The Register. Retrieved from: https://www.theregister.co.uk/2006/08/15/beer_diapers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provost, F. & Fawcett, T. (2013). \"Data Science for Business\". O'Reilly. Sebastopol, CA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O'Dea, P., Griffith, J., O'Riordan, C. \"Combining Feature Selection and Neural Networks for Solving Classification Problems\". National University of Ireland. Galway, Galway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
